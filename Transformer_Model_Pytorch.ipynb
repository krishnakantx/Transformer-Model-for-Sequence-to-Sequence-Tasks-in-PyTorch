{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ORE8Or39EmQ",
        "outputId": "ff880530-6222-4d20-ce5c-20167fd4665c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"Running on {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Running on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "s219IyXlMHhu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads):\n",
        "        super(AttentionHead, self).__init__()\n",
        "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = model_dim // num_heads\n",
        "\n",
        "        self.query_layer = nn.Linear(model_dim, model_dim)\n",
        "        self.key_layer = nn.Linear(model_dim, model_dim)\n",
        "        self.value_layer = nn.Linear(model_dim, model_dim)\n",
        "        self.output_layer = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def compute_attention(self, query, key, value, mask=None):\n",
        "            # Compute attention scores (dot product of query and key)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Check if mask is provided\n",
        "        if mask is not None:\n",
        "            # Expand mask to match the dimensions of attention scores (batch_size, num_heads, seq_len, seq_len)\n",
        "            mask = mask.unsqueeze(1).repeat(1, query.size(1), 1, 1)\n",
        "\n",
        "            # Apply the mask to the scores (masked_fill expects the mask to have the same shape as scores)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Compute the attention probabilities\n",
        "        attention_probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiply the attention probabilities with the value tensor\n",
        "        output = torch.matmul(attention_probs, value)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, tensor):\n",
        "        batch_size, seq_len, model_dim = tensor.size()\n",
        "        return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def merge_heads(self, tensor):\n",
        "        batch_size, num_heads, seq_len, head_dim = tensor.size()\n",
        "        return tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        query = self.split_heads(self.query_layer(query))\n",
        "        key = self.split_heads(self.key_layer(key))\n",
        "        value = self.split_heads(self.value_layer(value))\n",
        "\n",
        "        attention_output = self.compute_attention(query, key, value, mask)\n",
        "        output = self.output_layer(self.merge_heads(attention_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "wLtXF1nAMKo4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, model_dim, hidden_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(model_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, model_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "4WSwGP0fMNMA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, model_dim, max_len):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        pos_enc = torch.zeros(max_len, model_dim)\n",
        "        positions = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * -(math.log(10000.0) / model_dim))\n",
        "\n",
        "        pos_enc[:, 0::2] = torch.sin(positions * div_term)\n",
        "        pos_enc[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        self.register_buffer('pos_enc', pos_enc.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_enc[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "nVxO97hAMOrj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hidden_dim, dropout_rate):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention = AttentionHead(model_dim, num_heads)\n",
        "        self.feed_forward = FeedForwardNetwork(model_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(model_dim)\n",
        "        self.norm2 = nn.LayerNorm(model_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, encoder_input, src_mask):\n",
        "        attention_out = self.self_attention(encoder_input, encoder_input, encoder_input, src_mask)\n",
        "        encoder_input = self.norm1(encoder_input + self.dropout(attention_out))\n",
        "        ff_out = self.feed_forward(encoder_input)\n",
        "        encoder_input = self.norm2(encoder_input + self.dropout(ff_out))\n",
        "        return encoder_input"
      ],
      "metadata": {
        "id": "JTN_dspjMQd-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, model_dim, num_heads, hidden_dim, dropout_rate):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attention = AttentionHead(model_dim, num_heads)\n",
        "        self.cross_attention = AttentionHead(model_dim, num_heads)\n",
        "        self.feed_forward = FeedForwardNetwork(model_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(model_dim)\n",
        "        self.norm2 = nn.LayerNorm(model_dim)\n",
        "        self.norm3 = nn.LayerNorm(model_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output, src_mask, tgt_mask):\n",
        "        attention_out = self.self_attention(decoder_input, decoder_input, decoder_input, tgt_mask)\n",
        "        decoder_input = self.norm1(decoder_input + self.dropout(attention_out))\n",
        "        cross_attention_out = self.cross_attention(decoder_input, encoder_output, encoder_output, src_mask)\n",
        "        decoder_input = self.norm2(decoder_input + self.dropout(cross_attention_out))\n",
        "        ff_out = self.feed_forward(decoder_input)\n",
        "        decoder_input = self.norm3(decoder_input + self.dropout(ff_out))\n",
        "        return decoder_input"
      ],
      "metadata": {
        "id": "5eDbjjXGMRLG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, model_dim, num_heads, num_layers, hidden_dim, max_len, dropout_rate):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, model_dim)\n",
        "        self.decoder_embedding = nn.Embedding(output_vocab_size, model_dim)\n",
        "        self.positional_embedding = PositionalEmbedding(model_dim, max_len)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([EncoderBlock(model_dim, num_heads, hidden_dim, dropout_rate) for _ in range(num_layers)])\n",
        "        self.decoder_blocks = nn.ModuleList([DecoderBlock(model_dim, num_heads, hidden_dim, dropout_rate) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(model_dim, output_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def generate_mask(self, source, target):\n",
        "        # Ensure masks are created on the same device as the inputs\n",
        "        src_mask = (source != 0).unsqueeze(1).to(source.device)\n",
        "        tgt_mask = (target != 0).unsqueeze(1).to(target.device)\n",
        "\n",
        "        seq_len = target.size(1)\n",
        "        no_peak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool().to(target.device)\n",
        "        tgt_mask = tgt_mask & no_peak_mask\n",
        "\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "    def forward(self, src_input, tgt_input):\n",
        "        src_mask, tgt_mask = self.generate_mask(src_input, tgt_input)\n",
        "        src_embedded = self.dropout(self.positional_embedding(self.encoder_embedding(src_input)))\n",
        "        tgt_embedded = self.dropout(self.positional_embedding(self.decoder_embedding(tgt_input)))\n",
        "\n",
        "        encoder_output = src_embedded\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            encoder_output = encoder_block(encoder_output, src_mask)\n",
        "\n",
        "        decoder_output = tgt_embedded\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            decoder_output = decoder_block(decoder_output, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc_out(decoder_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "OZ4xpf0WMUKc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "input_vocab_size = 5000\n",
        "output_vocab_size = 5000\n",
        "model_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_dim = 2048\n",
        "max_len = 100\n",
        "dropout_rate = 0.1\n",
        "\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model and move it to GPU (if available)\n",
        "transformer_model = TransformerModel(input_vocab_size, output_vocab_size, model_dim, num_heads, num_layers, hidden_dim, max_len, dropout_rate).to(device)\n",
        "\n",
        "# Generate random sample data and move it to GPU (if available)\n",
        "src_batch = torch.randint(1, input_vocab_size, (64, max_len)).to(device)  # (batch_size, seq_length)\n",
        "tgt_batch = torch.randint(1, output_vocab_size, (64, max_len)).to(device)  # (batch_size, seq_length)\n",
        "\n",
        "# Set loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Training loop\n",
        "transformer_model.train()\n",
        "\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass with data moved to GPU\n",
        "    predictions = transformer_model(src_batch, tgt_batch[:, :-1])\n",
        "\n",
        "    # Compute loss with data on GPU\n",
        "    loss = loss_fn(predictions.contiguous().view(-1, output_vocab_size), tgt_batch[:, 1:].contiguous().view(-1))\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FvJcwMT9MHc",
        "outputId": "be0850e6-e04a-4678-9e2e-1c38516ef9bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.684684753417969\n",
            "Epoch 2, Loss: 8.552639961242676\n",
            "Epoch 3, Loss: 8.483640670776367\n",
            "Epoch 4, Loss: 8.42927360534668\n",
            "Epoch 5, Loss: 8.371991157531738\n",
            "Epoch 6, Loss: 8.30825424194336\n",
            "Epoch 7, Loss: 8.227605819702148\n",
            "Epoch 8, Loss: 8.151398658752441\n",
            "Epoch 9, Loss: 8.06497573852539\n",
            "Epoch 10, Loss: 7.992325782775879\n",
            "Epoch 11, Loss: 7.903415679931641\n",
            "Epoch 12, Loss: 7.822192192077637\n",
            "Epoch 13, Loss: 7.738828659057617\n",
            "Epoch 14, Loss: 7.654994010925293\n",
            "Epoch 15, Loss: 7.575342655181885\n",
            "Epoch 16, Loss: 7.493861198425293\n",
            "Epoch 17, Loss: 7.406813621520996\n",
            "Epoch 18, Loss: 7.333458423614502\n",
            "Epoch 19, Loss: 7.247169017791748\n",
            "Epoch 20, Loss: 7.165889739990234\n",
            "Epoch 21, Loss: 7.090582847595215\n",
            "Epoch 22, Loss: 7.0098371505737305\n",
            "Epoch 23, Loss: 6.936218738555908\n",
            "Epoch 24, Loss: 6.860867023468018\n",
            "Epoch 25, Loss: 6.768840789794922\n",
            "Epoch 26, Loss: 6.697455406188965\n",
            "Epoch 27, Loss: 6.620323181152344\n",
            "Epoch 28, Loss: 6.553584575653076\n",
            "Epoch 29, Loss: 6.484265327453613\n",
            "Epoch 30, Loss: 6.416414737701416\n",
            "Epoch 31, Loss: 6.3475565910339355\n",
            "Epoch 32, Loss: 6.2734150886535645\n",
            "Epoch 33, Loss: 6.206218242645264\n",
            "Epoch 34, Loss: 6.142878532409668\n",
            "Epoch 35, Loss: 6.069670677185059\n",
            "Epoch 36, Loss: 6.007379055023193\n",
            "Epoch 37, Loss: 5.945221424102783\n",
            "Epoch 38, Loss: 5.872042179107666\n",
            "Epoch 39, Loss: 5.8150506019592285\n",
            "Epoch 40, Loss: 5.747838973999023\n",
            "Epoch 41, Loss: 5.684561252593994\n",
            "Epoch 42, Loss: 5.62236213684082\n",
            "Epoch 43, Loss: 5.565959930419922\n",
            "Epoch 44, Loss: 5.499027252197266\n",
            "Epoch 45, Loss: 5.439728260040283\n",
            "Epoch 46, Loss: 5.377691268920898\n",
            "Epoch 47, Loss: 5.319768905639648\n",
            "Epoch 48, Loss: 5.264151573181152\n",
            "Epoch 49, Loss: 5.209941387176514\n",
            "Epoch 50, Loss: 5.153867244720459\n",
            "Epoch 51, Loss: 5.097487449645996\n",
            "Epoch 52, Loss: 5.03849983215332\n",
            "Epoch 53, Loss: 4.988828659057617\n",
            "Epoch 54, Loss: 4.930414199829102\n",
            "Epoch 55, Loss: 4.866606712341309\n",
            "Epoch 56, Loss: 4.8166704177856445\n",
            "Epoch 57, Loss: 4.770045280456543\n",
            "Epoch 58, Loss: 4.712265968322754\n",
            "Epoch 59, Loss: 4.663662910461426\n",
            "Epoch 60, Loss: 4.605467796325684\n",
            "Epoch 61, Loss: 4.559380531311035\n",
            "Epoch 62, Loss: 4.503207683563232\n",
            "Epoch 63, Loss: 4.447695732116699\n",
            "Epoch 64, Loss: 4.397375106811523\n",
            "Epoch 65, Loss: 4.345190525054932\n",
            "Epoch 66, Loss: 4.300312519073486\n",
            "Epoch 67, Loss: 4.248775482177734\n",
            "Epoch 68, Loss: 4.195982456207275\n",
            "Epoch 69, Loss: 4.14597225189209\n",
            "Epoch 70, Loss: 4.101710796356201\n",
            "Epoch 71, Loss: 4.052351474761963\n",
            "Epoch 72, Loss: 4.002959728240967\n",
            "Epoch 73, Loss: 3.9490301609039307\n",
            "Epoch 74, Loss: 3.9130067825317383\n",
            "Epoch 75, Loss: 3.8539912700653076\n",
            "Epoch 76, Loss: 3.814990520477295\n",
            "Epoch 77, Loss: 3.762563467025757\n",
            "Epoch 78, Loss: 3.7115938663482666\n",
            "Epoch 79, Loss: 3.6703362464904785\n",
            "Epoch 80, Loss: 3.620347738265991\n",
            "Epoch 81, Loss: 3.5711965560913086\n",
            "Epoch 82, Loss: 3.536954879760742\n",
            "Epoch 83, Loss: 3.496626377105713\n",
            "Epoch 84, Loss: 3.4308366775512695\n",
            "Epoch 85, Loss: 3.397775411605835\n",
            "Epoch 86, Loss: 3.343752145767212\n",
            "Epoch 87, Loss: 3.304546594619751\n",
            "Epoch 88, Loss: 3.2581841945648193\n",
            "Epoch 89, Loss: 3.2155983448028564\n",
            "Epoch 90, Loss: 3.1622862815856934\n",
            "Epoch 91, Loss: 3.1296157836914062\n",
            "Epoch 92, Loss: 3.0821471214294434\n",
            "Epoch 93, Loss: 3.0420477390289307\n",
            "Epoch 94, Loss: 2.9945006370544434\n",
            "Epoch 95, Loss: 2.9530625343322754\n",
            "Epoch 96, Loss: 2.9155125617980957\n",
            "Epoch 97, Loss: 2.8715500831604004\n",
            "Epoch 98, Loss: 2.8180325031280518\n",
            "Epoch 99, Loss: 2.7850403785705566\n",
            "Epoch 100, Loss: 2.7378995418548584\n",
            "Epoch 101, Loss: 2.6965980529785156\n",
            "Epoch 102, Loss: 2.6533801555633545\n",
            "Epoch 103, Loss: 2.611931085586548\n",
            "Epoch 104, Loss: 2.5723190307617188\n",
            "Epoch 105, Loss: 2.5307929515838623\n",
            "Epoch 106, Loss: 2.497591018676758\n",
            "Epoch 107, Loss: 2.4532949924468994\n",
            "Epoch 108, Loss: 2.417640447616577\n",
            "Epoch 109, Loss: 2.367298126220703\n",
            "Epoch 110, Loss: 2.335427761077881\n",
            "Epoch 111, Loss: 2.29927134513855\n",
            "Epoch 112, Loss: 2.259190320968628\n",
            "Epoch 113, Loss: 2.2117321491241455\n",
            "Epoch 114, Loss: 2.17258358001709\n",
            "Epoch 115, Loss: 2.139442205429077\n",
            "Epoch 116, Loss: 2.10237717628479\n",
            "Epoch 117, Loss: 2.0631964206695557\n",
            "Epoch 118, Loss: 2.028715133666992\n",
            "Epoch 119, Loss: 1.9896304607391357\n",
            "Epoch 120, Loss: 1.9501210451126099\n",
            "Epoch 121, Loss: 1.9196698665618896\n",
            "Epoch 122, Loss: 1.8872348070144653\n",
            "Epoch 123, Loss: 1.8562350273132324\n",
            "Epoch 124, Loss: 1.8139607906341553\n",
            "Epoch 125, Loss: 1.7811806201934814\n",
            "Epoch 126, Loss: 1.7417995929718018\n",
            "Epoch 127, Loss: 1.7122622728347778\n",
            "Epoch 128, Loss: 1.674249529838562\n",
            "Epoch 129, Loss: 1.6507247686386108\n",
            "Epoch 130, Loss: 1.6018952131271362\n",
            "Epoch 131, Loss: 1.5723832845687866\n",
            "Epoch 132, Loss: 1.5447146892547607\n",
            "Epoch 133, Loss: 1.5149730443954468\n",
            "Epoch 134, Loss: 1.4778163433074951\n",
            "Epoch 135, Loss: 1.4480047225952148\n",
            "Epoch 136, Loss: 1.4230388402938843\n",
            "Epoch 137, Loss: 1.3882148265838623\n",
            "Epoch 138, Loss: 1.359151840209961\n",
            "Epoch 139, Loss: 1.3323112726211548\n",
            "Epoch 140, Loss: 1.3019105195999146\n",
            "Epoch 141, Loss: 1.2722214460372925\n",
            "Epoch 142, Loss: 1.2422250509262085\n",
            "Epoch 143, Loss: 1.2168896198272705\n",
            "Epoch 144, Loss: 1.1881757974624634\n",
            "Epoch 145, Loss: 1.161211371421814\n",
            "Epoch 146, Loss: 1.1363269090652466\n",
            "Epoch 147, Loss: 1.1096996068954468\n",
            "Epoch 148, Loss: 1.0821460485458374\n",
            "Epoch 149, Loss: 1.0624672174453735\n",
            "Epoch 150, Loss: 1.036933183670044\n",
            "Epoch 151, Loss: 1.0112876892089844\n",
            "Epoch 152, Loss: 0.9825732111930847\n",
            "Epoch 153, Loss: 0.9612199664115906\n",
            "Epoch 154, Loss: 0.9370948672294617\n",
            "Epoch 155, Loss: 0.9173486232757568\n",
            "Epoch 156, Loss: 0.8985056281089783\n",
            "Epoch 157, Loss: 0.8756987452507019\n",
            "Epoch 158, Loss: 0.8544179797172546\n",
            "Epoch 159, Loss: 0.8350432515144348\n",
            "Epoch 160, Loss: 0.81103515625\n",
            "Epoch 161, Loss: 0.7935115098953247\n",
            "Epoch 162, Loss: 0.7786871194839478\n",
            "Epoch 163, Loss: 0.7581939101219177\n",
            "Epoch 164, Loss: 0.7376030683517456\n",
            "Epoch 165, Loss: 0.7220368981361389\n",
            "Epoch 166, Loss: 0.7074074745178223\n",
            "Epoch 167, Loss: 0.6821262836456299\n",
            "Epoch 168, Loss: 0.6677100658416748\n",
            "Epoch 169, Loss: 0.6533188223838806\n",
            "Epoch 170, Loss: 0.6338800191879272\n",
            "Epoch 171, Loss: 0.6165647506713867\n",
            "Epoch 172, Loss: 0.6049144268035889\n",
            "Epoch 173, Loss: 0.5916780233383179\n",
            "Epoch 174, Loss: 0.573306679725647\n",
            "Epoch 175, Loss: 0.563726007938385\n",
            "Epoch 176, Loss: 0.5473731160163879\n",
            "Epoch 177, Loss: 0.5396157503128052\n",
            "Epoch 178, Loss: 0.5221641063690186\n",
            "Epoch 179, Loss: 0.5091245770454407\n",
            "Epoch 180, Loss: 0.4979204833507538\n",
            "Epoch 181, Loss: 0.4863991439342499\n",
            "Epoch 182, Loss: 0.4756803810596466\n",
            "Epoch 183, Loss: 0.4625397026538849\n",
            "Epoch 184, Loss: 0.452801913022995\n",
            "Epoch 185, Loss: 0.44393253326416016\n",
            "Epoch 186, Loss: 0.4298023283481598\n",
            "Epoch 187, Loss: 0.4204493463039398\n",
            "Epoch 188, Loss: 0.4112296402454376\n",
            "Epoch 189, Loss: 0.4002547264099121\n",
            "Epoch 190, Loss: 0.3929676115512848\n",
            "Epoch 191, Loss: 0.3788895905017853\n",
            "Epoch 192, Loss: 0.37537503242492676\n",
            "Epoch 193, Loss: 0.36280032992362976\n",
            "Epoch 194, Loss: 0.35537612438201904\n",
            "Epoch 195, Loss: 0.34753113985061646\n",
            "Epoch 196, Loss: 0.340494304895401\n",
            "Epoch 197, Loss: 0.33292439579963684\n",
            "Epoch 198, Loss: 0.32683631777763367\n",
            "Epoch 199, Loss: 0.31772366166114807\n",
            "Epoch 200, Loss: 0.3118463456630707\n",
            "Epoch 201, Loss: 0.30522939562797546\n",
            "Epoch 202, Loss: 0.29665178060531616\n",
            "Epoch 203, Loss: 0.29056400060653687\n",
            "Epoch 204, Loss: 0.2842179238796234\n",
            "Epoch 205, Loss: 0.2796211838722229\n",
            "Epoch 206, Loss: 0.2739870250225067\n",
            "Epoch 207, Loss: 0.26669812202453613\n",
            "Epoch 208, Loss: 0.26219984889030457\n",
            "Epoch 209, Loss: 0.25723251700401306\n",
            "Epoch 210, Loss: 0.24931609630584717\n",
            "Epoch 211, Loss: 0.2463970184326172\n",
            "Epoch 212, Loss: 0.24132782220840454\n",
            "Epoch 213, Loss: 0.23578189313411713\n",
            "Epoch 214, Loss: 0.23290202021598816\n",
            "Epoch 215, Loss: 0.22645919024944305\n",
            "Epoch 216, Loss: 0.22149236500263214\n",
            "Epoch 217, Loss: 0.21720196306705475\n",
            "Epoch 218, Loss: 0.2133731245994568\n",
            "Epoch 219, Loss: 0.2074449509382248\n",
            "Epoch 220, Loss: 0.20380237698554993\n",
            "Epoch 221, Loss: 0.2016182839870453\n",
            "Epoch 222, Loss: 0.19647203385829926\n",
            "Epoch 223, Loss: 0.19377627968788147\n",
            "Epoch 224, Loss: 0.18955335021018982\n",
            "Epoch 225, Loss: 0.1862458884716034\n",
            "Epoch 226, Loss: 0.18137127161026\n",
            "Epoch 227, Loss: 0.1793789118528366\n",
            "Epoch 228, Loss: 0.17461757361888885\n",
            "Epoch 229, Loss: 0.17346179485321045\n",
            "Epoch 230, Loss: 0.16923385858535767\n",
            "Epoch 231, Loss: 0.1654314547777176\n",
            "Epoch 232, Loss: 0.16300825774669647\n",
            "Epoch 233, Loss: 0.16039451956748962\n",
            "Epoch 234, Loss: 0.15595507621765137\n",
            "Epoch 235, Loss: 0.1551484614610672\n",
            "Epoch 236, Loss: 0.15308085083961487\n",
            "Epoch 237, Loss: 0.14911237359046936\n",
            "Epoch 238, Loss: 0.14588071405887604\n",
            "Epoch 239, Loss: 0.14485634863376617\n",
            "Epoch 240, Loss: 0.14124247431755066\n",
            "Epoch 241, Loss: 0.1385420709848404\n",
            "Epoch 242, Loss: 0.1364237517118454\n",
            "Epoch 243, Loss: 0.133931964635849\n",
            "Epoch 244, Loss: 0.13170704245567322\n",
            "Epoch 245, Loss: 0.1293415129184723\n",
            "Epoch 246, Loss: 0.12735417485237122\n",
            "Epoch 247, Loss: 0.12618888914585114\n",
            "Epoch 248, Loss: 0.12432407587766647\n",
            "Epoch 249, Loss: 0.12098672986030579\n",
            "Epoch 250, Loss: 0.12014725059270859\n",
            "Epoch 251, Loss: 0.11678880453109741\n",
            "Epoch 252, Loss: 0.11549818515777588\n",
            "Epoch 253, Loss: 0.113461434841156\n",
            "Epoch 254, Loss: 0.11082016676664352\n",
            "Epoch 255, Loss: 0.10960013419389725\n",
            "Epoch 256, Loss: 0.10797064006328583\n",
            "Epoch 257, Loss: 0.10604409128427505\n",
            "Epoch 258, Loss: 0.10451560467481613\n",
            "Epoch 259, Loss: 0.10268809646368027\n",
            "Epoch 260, Loss: 0.10137725621461868\n",
            "Epoch 261, Loss: 0.10002241283655167\n",
            "Epoch 262, Loss: 0.0981476828455925\n",
            "Epoch 263, Loss: 0.09629743546247482\n",
            "Epoch 264, Loss: 0.09488023072481155\n",
            "Epoch 265, Loss: 0.09379386156797409\n",
            "Epoch 266, Loss: 0.09223223477602005\n",
            "Epoch 267, Loss: 0.09114525467157364\n",
            "Epoch 268, Loss: 0.08966231346130371\n",
            "Epoch 269, Loss: 0.08730785548686981\n",
            "Epoch 270, Loss: 0.08753065764904022\n",
            "Epoch 271, Loss: 0.08525150269269943\n",
            "Epoch 272, Loss: 0.08456290513277054\n",
            "Epoch 273, Loss: 0.08260129392147064\n",
            "Epoch 274, Loss: 0.08068183809518814\n",
            "Epoch 275, Loss: 0.0801471546292305\n",
            "Epoch 276, Loss: 0.07931536436080933\n",
            "Epoch 277, Loss: 0.07798413187265396\n",
            "Epoch 278, Loss: 0.0774126723408699\n",
            "Epoch 279, Loss: 0.07574397325515747\n",
            "Epoch 280, Loss: 0.07449531555175781\n",
            "Epoch 281, Loss: 0.07403276860713959\n",
            "Epoch 282, Loss: 0.07264941930770874\n",
            "Epoch 283, Loss: 0.07132425904273987\n",
            "Epoch 284, Loss: 0.07064260542392731\n",
            "Epoch 285, Loss: 0.06946597248315811\n",
            "Epoch 286, Loss: 0.06872466206550598\n",
            "Epoch 287, Loss: 0.06697496771812439\n",
            "Epoch 288, Loss: 0.06664644181728363\n",
            "Epoch 289, Loss: 0.0657750591635704\n",
            "Epoch 290, Loss: 0.06416942924261093\n",
            "Epoch 291, Loss: 0.06404224783182144\n",
            "Epoch 292, Loss: 0.062160830944776535\n",
            "Epoch 293, Loss: 0.061723265796899796\n",
            "Epoch 294, Loss: 0.06114882230758667\n",
            "Epoch 295, Loss: 0.0604364238679409\n",
            "Epoch 296, Loss: 0.059904444962739944\n",
            "Epoch 297, Loss: 0.05889974907040596\n",
            "Epoch 298, Loss: 0.0578119233250618\n",
            "Epoch 299, Loss: 0.057337746024131775\n",
            "Epoch 300, Loss: 0.05625007301568985\n",
            "Epoch 301, Loss: 0.05528023838996887\n",
            "Epoch 302, Loss: 0.05491474270820618\n",
            "Epoch 303, Loss: 0.05419391393661499\n",
            "Epoch 304, Loss: 0.05309830233454704\n",
            "Epoch 305, Loss: 0.05280431732535362\n",
            "Epoch 306, Loss: 0.05190206691622734\n",
            "Epoch 307, Loss: 0.051358193159103394\n",
            "Epoch 308, Loss: 0.05089174211025238\n",
            "Epoch 309, Loss: 0.04971645027399063\n",
            "Epoch 310, Loss: 0.04909469187259674\n",
            "Epoch 311, Loss: 0.04838472977280617\n",
            "Epoch 312, Loss: 0.04819003492593765\n",
            "Epoch 313, Loss: 0.046999115496873856\n",
            "Epoch 314, Loss: 0.046683866530656815\n",
            "Epoch 315, Loss: 0.046270351856946945\n",
            "Epoch 316, Loss: 0.04543966054916382\n",
            "Epoch 317, Loss: 0.04481803625822067\n",
            "Epoch 318, Loss: 0.04466058313846588\n",
            "Epoch 319, Loss: 0.04379519075155258\n",
            "Epoch 320, Loss: 0.04274692013859749\n",
            "Epoch 321, Loss: 0.042600538581609726\n",
            "Epoch 322, Loss: 0.042712148278951645\n",
            "Epoch 323, Loss: 0.041699063032865524\n",
            "Epoch 324, Loss: 0.041101060807704926\n",
            "Epoch 325, Loss: 0.04040835052728653\n",
            "Epoch 326, Loss: 0.039959508925676346\n",
            "Epoch 327, Loss: 0.03983323648571968\n",
            "Epoch 328, Loss: 0.03877285122871399\n",
            "Epoch 329, Loss: 0.038285598158836365\n",
            "Epoch 330, Loss: 0.03781436011195183\n",
            "Epoch 331, Loss: 0.03736034780740738\n",
            "Epoch 332, Loss: 0.0371360257267952\n",
            "Epoch 333, Loss: 0.03650687634944916\n",
            "Epoch 334, Loss: 0.03616635128855705\n",
            "Epoch 335, Loss: 0.035842038691043854\n",
            "Epoch 336, Loss: 0.034982532262802124\n",
            "Epoch 337, Loss: 0.03457967936992645\n",
            "Epoch 338, Loss: 0.03416081517934799\n",
            "Epoch 339, Loss: 0.033914193511009216\n",
            "Epoch 340, Loss: 0.03374263271689415\n",
            "Epoch 341, Loss: 0.03309931233525276\n",
            "Epoch 342, Loss: 0.03271900862455368\n",
            "Epoch 343, Loss: 0.03221767768263817\n",
            "Epoch 344, Loss: 0.03184746950864792\n",
            "Epoch 345, Loss: 0.03165101259946823\n",
            "Epoch 346, Loss: 0.031085899099707603\n",
            "Epoch 347, Loss: 0.030809270218014717\n",
            "Epoch 348, Loss: 0.030287183821201324\n",
            "Epoch 349, Loss: 0.03005880117416382\n",
            "Epoch 350, Loss: 0.029666580259799957\n",
            "Epoch 351, Loss: 0.02901644818484783\n",
            "Epoch 352, Loss: 0.02922835201025009\n",
            "Epoch 353, Loss: 0.028683749958872795\n",
            "Epoch 354, Loss: 0.028124239295721054\n",
            "Epoch 355, Loss: 0.02793697454035282\n",
            "Epoch 356, Loss: 0.027365850284695625\n",
            "Epoch 357, Loss: 0.027131246402859688\n",
            "Epoch 358, Loss: 0.026993850246071815\n",
            "Epoch 359, Loss: 0.026362553238868713\n",
            "Epoch 360, Loss: 0.026115301996469498\n",
            "Epoch 361, Loss: 0.025500988587737083\n",
            "Epoch 362, Loss: 0.025665249675512314\n",
            "Epoch 363, Loss: 0.025391556322574615\n",
            "Epoch 364, Loss: 0.025136901065707207\n",
            "Epoch 365, Loss: 0.02473069168627262\n",
            "Epoch 366, Loss: 0.024298544973134995\n",
            "Epoch 367, Loss: 0.02410033531486988\n",
            "Epoch 368, Loss: 0.023808715865015984\n",
            "Epoch 369, Loss: 0.023469768464565277\n",
            "Epoch 370, Loss: 0.023346416652202606\n",
            "Epoch 371, Loss: 0.023025650531053543\n",
            "Epoch 372, Loss: 0.022538619115948677\n",
            "Epoch 373, Loss: 0.022356458008289337\n",
            "Epoch 374, Loss: 0.022285955026745796\n",
            "Epoch 375, Loss: 0.021906277164816856\n",
            "Epoch 376, Loss: 0.02154863066971302\n",
            "Epoch 377, Loss: 0.021522480994462967\n",
            "Epoch 378, Loss: 0.021172408014535904\n",
            "Epoch 379, Loss: 0.020854968577623367\n",
            "Epoch 380, Loss: 0.020562302321195602\n",
            "Epoch 381, Loss: 0.020561285316944122\n",
            "Epoch 382, Loss: 0.020153583958745003\n",
            "Epoch 383, Loss: 0.019850943237543106\n",
            "Epoch 384, Loss: 0.019699115306138992\n",
            "Epoch 385, Loss: 0.01943313516676426\n",
            "Epoch 386, Loss: 0.019299492239952087\n",
            "Epoch 387, Loss: 0.019158193841576576\n",
            "Epoch 388, Loss: 0.018915699794888496\n",
            "Epoch 389, Loss: 0.018575144931674004\n",
            "Epoch 390, Loss: 0.01839291676878929\n",
            "Epoch 391, Loss: 0.018184660002589226\n",
            "Epoch 392, Loss: 0.018002858385443687\n",
            "Epoch 393, Loss: 0.01784270443022251\n",
            "Epoch 394, Loss: 0.017621006816625595\n",
            "Epoch 395, Loss: 0.01746397651731968\n",
            "Epoch 396, Loss: 0.017153572291135788\n",
            "Epoch 397, Loss: 0.016996249556541443\n",
            "Epoch 398, Loss: 0.016760362312197685\n",
            "Epoch 399, Loss: 0.016535375267267227\n",
            "Epoch 400, Loss: 0.016490818932652473\n",
            "Epoch 401, Loss: 0.016183750703930855\n",
            "Epoch 402, Loss: 0.016055742278695107\n",
            "Epoch 403, Loss: 0.015894964337348938\n",
            "Epoch 404, Loss: 0.01568303070962429\n",
            "Epoch 405, Loss: 0.015562194399535656\n",
            "Epoch 406, Loss: 0.015273049473762512\n",
            "Epoch 407, Loss: 0.015215289779007435\n",
            "Epoch 408, Loss: 0.014991008676588535\n",
            "Epoch 409, Loss: 0.014813989400863647\n",
            "Epoch 410, Loss: 0.014609537087380886\n",
            "Epoch 411, Loss: 0.014655827544629574\n",
            "Epoch 412, Loss: 0.014279313385486603\n",
            "Epoch 413, Loss: 0.014173432253301144\n",
            "Epoch 414, Loss: 0.01402431819587946\n",
            "Epoch 415, Loss: 0.013997508212924004\n",
            "Epoch 416, Loss: 0.01367129571735859\n",
            "Epoch 417, Loss: 0.013491849415004253\n",
            "Epoch 418, Loss: 0.013442521914839745\n",
            "Epoch 419, Loss: 0.01318742148578167\n",
            "Epoch 420, Loss: 0.013200784102082253\n",
            "Epoch 421, Loss: 0.012943449430167675\n",
            "Epoch 422, Loss: 0.012867406010627747\n",
            "Epoch 423, Loss: 0.012666176073253155\n",
            "Epoch 424, Loss: 0.0125389089807868\n",
            "Epoch 425, Loss: 0.012289972975850105\n",
            "Epoch 426, Loss: 0.01228174939751625\n",
            "Epoch 427, Loss: 0.012105821631848812\n",
            "Epoch 428, Loss: 0.01200816873461008\n",
            "Epoch 429, Loss: 0.011883660219609737\n",
            "Epoch 430, Loss: 0.011776602827012539\n",
            "Epoch 431, Loss: 0.011535318568348885\n",
            "Epoch 432, Loss: 0.011520007625222206\n",
            "Epoch 433, Loss: 0.01134183444082737\n",
            "Epoch 434, Loss: 0.011294610798358917\n",
            "Epoch 435, Loss: 0.011049027554690838\n",
            "Epoch 436, Loss: 0.01097094640135765\n",
            "Epoch 437, Loss: 0.0109119126573205\n",
            "Epoch 438, Loss: 0.010788661427795887\n",
            "Epoch 439, Loss: 0.010612653568387032\n",
            "Epoch 440, Loss: 0.010594421997666359\n",
            "Epoch 441, Loss: 0.010382729582488537\n",
            "Epoch 442, Loss: 0.010326444171369076\n",
            "Epoch 443, Loss: 0.010150057263672352\n",
            "Epoch 444, Loss: 0.010142280720174313\n",
            "Epoch 445, Loss: 0.009937915951013565\n",
            "Epoch 446, Loss: 0.009802334941923618\n",
            "Epoch 447, Loss: 0.009714232757687569\n",
            "Epoch 448, Loss: 0.009631725028157234\n",
            "Epoch 449, Loss: 0.009534765034914017\n",
            "Epoch 450, Loss: 0.00945819541811943\n",
            "Epoch 451, Loss: 0.009286870248615742\n",
            "Epoch 452, Loss: 0.009247745387256145\n",
            "Epoch 453, Loss: 0.009045828133821487\n",
            "Epoch 454, Loss: 0.009067192673683167\n",
            "Epoch 455, Loss: 0.008881459012627602\n",
            "Epoch 456, Loss: 0.008824337273836136\n",
            "Epoch 457, Loss: 0.008629150688648224\n",
            "Epoch 458, Loss: 0.008638990111649036\n",
            "Epoch 459, Loss: 0.008556528948247433\n",
            "Epoch 460, Loss: 0.008502071723341942\n",
            "Epoch 461, Loss: 0.008364124223589897\n",
            "Epoch 462, Loss: 0.008343366906046867\n",
            "Epoch 463, Loss: 0.008185881190001965\n",
            "Epoch 464, Loss: 0.008078359067440033\n",
            "Epoch 465, Loss: 0.007984097115695477\n",
            "Epoch 466, Loss: 0.007901065051555634\n",
            "Epoch 467, Loss: 0.007884996943175793\n",
            "Epoch 468, Loss: 0.007716027088463306\n",
            "Epoch 469, Loss: 0.007616379763931036\n",
            "Epoch 470, Loss: 0.007577429059892893\n",
            "Epoch 471, Loss: 0.0075300089083611965\n",
            "Epoch 472, Loss: 0.0073983194306492805\n",
            "Epoch 473, Loss: 0.0073380074463784695\n",
            "Epoch 474, Loss: 0.007245367858558893\n",
            "Epoch 475, Loss: 0.007181735243648291\n",
            "Epoch 476, Loss: 0.007108894642442465\n",
            "Epoch 477, Loss: 0.007076438516378403\n",
            "Epoch 478, Loss: 0.006958912592381239\n",
            "Epoch 479, Loss: 0.006854817736893892\n",
            "Epoch 480, Loss: 0.006870810873806477\n",
            "Epoch 481, Loss: 0.006801153067499399\n",
            "Epoch 482, Loss: 0.006622916553169489\n",
            "Epoch 483, Loss: 0.00656750937923789\n",
            "Epoch 484, Loss: 0.006558215245604515\n",
            "Epoch 485, Loss: 0.006437445990741253\n",
            "Epoch 486, Loss: 0.006329885683953762\n",
            "Epoch 487, Loss: 0.006375514902174473\n",
            "Epoch 488, Loss: 0.006250705569982529\n",
            "Epoch 489, Loss: 0.006187013816088438\n",
            "Epoch 490, Loss: 0.006119435653090477\n",
            "Epoch 491, Loss: 0.006085032131522894\n",
            "Epoch 492, Loss: 0.005981061607599258\n",
            "Epoch 493, Loss: 0.0058949110098183155\n",
            "Epoch 494, Loss: 0.005942271091043949\n",
            "Epoch 495, Loss: 0.005823823623359203\n",
            "Epoch 496, Loss: 0.005740954540669918\n",
            "Epoch 497, Loss: 0.005674132611602545\n",
            "Epoch 498, Loss: 0.005599637981504202\n",
            "Epoch 499, Loss: 0.005561070051044226\n",
            "Epoch 500, Loss: 0.0055105905048549175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "493uIkrT9V43"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}